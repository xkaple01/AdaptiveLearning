{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import astunparse\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from pybase64 import b64decode\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def __init__(self, detector) -> None:\n",
    "        self.detector = detector\n",
    "        self.task_ids = []\n",
    "        self.user_codes = []\n",
    "        self.hints = []\n",
    "        self.counter_task_solutions = defaultdict(lambda: 0)\n",
    "        self.counter_task_errors = defaultdict(lambda: 0)\n",
    "\n",
    "    def run(self):\n",
    "        task_order = 0\n",
    "        task_ids = []\n",
    "        with open('./solutions-ipython.csv') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "            next(csv_reader)\n",
    "    \n",
    "            for row in csv_reader:\n",
    "                task_id = row[1]\n",
    "                solution_encoded = row[2]\n",
    "                solution_decoded = b64decode(solution_encoded).decode(\"utf-8\")\n",
    "                \n",
    "                self.counter_task_solutions[task_order] += 1\n",
    "                self.detector.detect_and_show_hint(code=solution_decoded)\n",
    "\n",
    "                if self.detector.detected:\n",
    "                    self.counter_task_errors[task_order] += 1\n",
    "                    self.task_ids.append(task_id)\n",
    "                    self.user_codes.append(self.detector.user_code)\n",
    "                    self.hints.append(self.detector.hint)\n",
    "\n",
    "                if task_id not in task_ids:\n",
    "                    task_ids.append(task_id)\n",
    "                    task_order += 1\n",
    "\n",
    "        print(f\"Total number of detected errors: {len(self.hints)}\")\n",
    "        self.get_n_tasks_with_greater_than_10_errors()\n",
    "\n",
    "    def increment_counter(self, counter, task_id):\n",
    "        if task_id not in counter.keys():\n",
    "            counter[task_id] = 1\n",
    "        else:\n",
    "            counter[task_id] += 1\n",
    "    \n",
    "    def display_detected_error(self, error_idx):\n",
    "        print(\n",
    "            \"User code: \\n\",\n",
    "            self.user_codes[error_idx],\n",
    "        )\n",
    "        print(\"\\n------------------\\n\")\n",
    "        print(\n",
    "            \"Hint: \\n\",\n",
    "            self.hints[error_idx],\n",
    "        )\n",
    "\n",
    "    def get_n_tasks_with_greater_than_10_errors(self):\n",
    "        self.n_tasks_with_greater_than_10_errors = 0\n",
    "        for v in self.counter_task_errors.values():\n",
    "            if v > 10:\n",
    "                self.n_tasks_with_greater_than_10_errors += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicatedSegment:\n",
    "    def __init__(self, lines_top: List[int], col_top: int, lines_bottom: List[int], col_bottom: int):\n",
    "        self.lines_top = lines_top\n",
    "        self.col_top = col_top\n",
    "        self.lines_bottom = lines_bottom\n",
    "        self.col_bottom = col_bottom\n",
    "        self.lines = lines_top + lines_bottom\n",
    "\n",
    "class DuplicatedCodeDetector:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"congcongwang/gpt2_medium_fine_tuned_coder\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"congcongwang/gpt2_medium_fine_tuned_coder\")     \n",
    "        self.model.to(\"cuda:1\")\n",
    "        self.cosine_similarity = torch.nn.CosineSimilarity(dim=1)\n",
    "        self.similarity_threshold = 0.97\n",
    "        \n",
    "    def _encode_subtrees(self):\n",
    "        for node in self._nodes:\n",
    "            code_str = astunparse.unparse(node)[:-1]\n",
    "            input_ids = self.tokenizer.encode(\"<python> \" + code_str, return_tensors='pt')\n",
    "            n_tokens = input_ids.shape[-1]\n",
    "            \n",
    "            outputs = self.model.generate(\n",
    "                input_ids=input_ids.to(\"cuda:1\"),\n",
    "                max_length=n_tokens + 1,\n",
    "                num_return_sequences=1,\n",
    "                output_hidden_states=True,\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "\n",
    "            self._nodes_enc.append(outputs.hidden_states[0][-1][:, -1, :])\n",
    "            clear_output()  \n",
    "    \n",
    "    def _get_child_nodes(self, node: ast.AST) -> None:\n",
    "        self._nodes.extend(node.body)\n",
    "        for n in ast.iter_child_nodes(node):\n",
    "            if hasattr(n, \"body\"):\n",
    "                self._get_child_nodes(n)\n",
    "\n",
    "    def _get_duplicity_matrix_base(self, root_node: ast.AST) -> None:\n",
    "        self._get_child_nodes(root_node)\n",
    "        self._nodes.sort(key=lambda n: n.lineno)\n",
    "\n",
    "    def _get_mask_same_type(self):\n",
    "        n_nodes = len(self._nodes)\n",
    "        self._mask_same_type = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n",
    "        \n",
    "        for i in range(n_nodes):\n",
    "            for j in range(n_nodes):\n",
    "                if j < i:\n",
    "                    self._mask_same_type[i, j] = self._mask_same_type[j, i]\n",
    "                elif j == i:\n",
    "                    self._mask_same_type[i, j] = 1.0\n",
    "                else:\n",
    "                    self._mask_same_type[i, j] = float(type(self._nodes[i]) == type(self._nodes[j]))\n",
    "\n",
    "    def _get_mask_same_num_lines(self):\n",
    "        n_nodes = len(self._nodes)\n",
    "        self._mask_same_num_lines = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n",
    "        \n",
    "        for i in range(n_nodes):\n",
    "            for j in range(n_nodes):\n",
    "                if j < i:\n",
    "                    self._mask_same_num_lines[i, j] = self._mask_same_num_lines[j, i]\n",
    "                elif j == i:\n",
    "                    self._mask_same_num_lines[i, j] = 1.0\n",
    "                else:\n",
    "                    self._mask_same_num_lines[i, j] = len(self._get_node_lines_list(self._nodes[i])) == len(self._get_node_lines_list(self._nodes[j]))\n",
    "                    \n",
    "\n",
    "    def _compute_duplicity_matrix_entry(self, node_1, node_2):\n",
    "        sim = self.cosine_similarity(node_1,node_2).cpu().numpy()[0]\n",
    "        return sim\n",
    "    \n",
    "    def _compute_duplicity_matrix(self):\n",
    "        n_nodes = len(self._nodes)\n",
    "        self._duplicity_matrix = np.zeros((n_nodes, n_nodes), dtype=np.float32)\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(n_nodes):\n",
    "                if j < i:\n",
    "                    self._duplicity_matrix[i, j] = self._duplicity_matrix[j, i]\n",
    "                elif j == i:\n",
    "                    self._duplicity_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    self._duplicity_matrix[i, j] = self._compute_duplicity_matrix_entry(\n",
    "                        self._nodes_enc[i],\n",
    "                        self._nodes_enc[j]\n",
    "                    )\n",
    "\n",
    "    def _scale_duplicity_matrix(self):\n",
    "        min_similarity = np.amin(self._duplicity_matrix)\n",
    "        self._duplicity_matrix = (self._duplicity_matrix - min_similarity) / (1 - min_similarity)\n",
    "\n",
    "    def _apply_mask_same_type(self):\n",
    "        self._duplicity_matrix = np.multiply(self._duplicity_matrix, self._mask_same_type)\n",
    "\n",
    "    def _apply_mask_same_num_lines(self):\n",
    "        self._duplicity_matrix = np.multiply(self._duplicity_matrix, self._mask_same_num_lines)\n",
    "\n",
    "    def _apply_similarity_threshold(self):\n",
    "        self._duplicity_matrix = (self._duplicity_matrix > self.similarity_threshold).astype(float)\n",
    "\n",
    "    def _check_lines_already_highlighted(self, lines):\n",
    "        higlighted_lines = []\n",
    "        already_highlighted = False\n",
    "\n",
    "        for s in self._duplicated_code_segments:\n",
    "            higlighted_lines.extend(s.lines)\n",
    "\n",
    "        for l in lines:\n",
    "            if l in higlighted_lines:\n",
    "                already_highlighted = True\n",
    "                break\n",
    "        \n",
    "        return already_highlighted\n",
    "    \n",
    "    def _get_node_lines_list(self, node):\n",
    "        return list(range(node.lineno, node.end_lineno + 1))\n",
    "    \n",
    "    def _get_duplicated_code_segments(self):\n",
    "        n_nodes = len(self._nodes)\n",
    "\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(i+1, n_nodes):\n",
    "                if (self._duplicity_matrix[i][j] == 1.0\n",
    "                    and not self._check_lines_already_highlighted(self._get_node_lines_list(self._nodes[j]))):\n",
    "                    self._duplicated_code_segments.append(\n",
    "                        DuplicatedSegment(\n",
    "                            lines_top=self._get_node_lines_list(self._nodes[i]),\n",
    "                            col_top=self._nodes[i].col_offset, \n",
    "                            lines_bottom=self._get_node_lines_list(self._nodes[j]),\n",
    "                            col_bottom=self._nodes[j].col_offset,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    def _merge_neighbouring_code_segments(self):\n",
    "        n_segments = len(self._duplicated_code_segments)\n",
    "        for i in range(n_segments):\n",
    "            for j in range(i+1, n_segments):\n",
    "                if (self._duplicated_code_segments[i].lines_top[-1] + 1 == self._duplicated_code_segments[j].lines_top[0]\n",
    "                    and self._duplicated_code_segments[i].lines_bottom[-1] + 1 == self._duplicated_code_segments[j].lines_bottom[0]\n",
    "                    and self._duplicated_code_segments[i].col_top == self._duplicated_code_segments[j].col_top\n",
    "                    and self._duplicated_code_segments[i].col_bottom == self._duplicated_code_segments[j].col_bottom):\n",
    "\n",
    "                    self._duplicated_code_segments[i].lines_top += self._duplicated_code_segments[j].lines_top\n",
    "                    self._duplicated_code_segments[i].lines_bottom += self._duplicated_code_segments[j].lines_bottom\n",
    "                    self._duplicated_code_segments[i].lines += self._duplicated_code_segments[j].lines \n",
    "\n",
    "    def _remove_segment_idxs(self, segment_idxs_to_remove):\n",
    "        n_segments = len(self._duplicated_code_segments)\n",
    "        duplicated_code_segments = []\n",
    "        for i in range(n_segments):\n",
    "            if i not in segment_idxs_to_remove:\n",
    "                duplicated_code_segments.append(self._duplicated_code_segments[i])\n",
    "\n",
    "        self._duplicated_code_segments = duplicated_code_segments\n",
    "    \n",
    "    def _remove_overlapping_code_segments_top_top(self):\n",
    "        n_segments = len(self._duplicated_code_segments)\n",
    "        segment_idxs_to_remove = []\n",
    "        for i in range(n_segments):\n",
    "            for j in range(n_segments):\n",
    "                if i != j:\n",
    "                    n_lines_i = len(self._duplicated_code_segments[i].lines_top)\n",
    "                    n_lines_j = len(self._duplicated_code_segments[j].lines_top)\n",
    "                    smaller_segment_idx = i if n_lines_i < n_lines_j else j\n",
    "                    if len(set(self._duplicated_code_segments[i].lines_top) & set(self._duplicated_code_segments[j].lines_top)) > 0:\n",
    "                        segment_idxs_to_remove.append(smaller_segment_idx)\n",
    "\n",
    "        self._remove_segment_idxs(segment_idxs_to_remove)\n",
    "\n",
    "    def _remove_overlapping_code_segments_top_bottom(self):\n",
    "        n_segments = len(self._duplicated_code_segments)\n",
    "        segment_idxs_to_remove = []\n",
    "        for i in range(n_segments):\n",
    "            if len(set(self._duplicated_code_segments[i].lines_top) & set(self._duplicated_code_segments[i].lines_bottom)) > 0:\n",
    "                segment_idxs_to_remove.append(i)\n",
    "\n",
    "        self._remove_segment_idxs(segment_idxs_to_remove)\n",
    "\n",
    "    def _remove_too_short_code_segments(self):\n",
    "        n_segments = len(self._duplicated_code_segments)\n",
    "        segment_idxs_to_remove = []\n",
    "        for i in range(n_segments):\n",
    "            if len(self._duplicated_code_segments[i].lines_top) < 2:\n",
    "                segment_idxs_to_remove.append(i)\n",
    "\n",
    "        self._remove_segment_idxs(segment_idxs_to_remove)\n",
    "\n",
    "    def _compose_hint(self, code):\n",
    "        code_strings = code.split('\\n')\n",
    "        self.hint: List[str] = []\n",
    "        for lineno, code_string in enumerate(code_strings):\n",
    "            string_is_part_of_duplicated_segment = False\n",
    "            segment_idx = None\n",
    "            for s_idx in range(len(self._duplicated_code_segments)):\n",
    "                if lineno + 1 in self._duplicated_code_segments[s_idx].lines:\n",
    "                    string_is_part_of_duplicated_segment = True\n",
    "                    segment_idx = s_idx\n",
    "    \n",
    "            if string_is_part_of_duplicated_segment:\n",
    "                code_string += f\"               : {segment_idx}\"\n",
    "    \n",
    "            self.hint.append(code_string)\n",
    "\n",
    "        self.hint: str = \"\\n\".join(self.hint)\n",
    "    \n",
    "    def _check_detected(self):\n",
    "        self.detected = len(self._duplicated_code_segments) > 0\n",
    "    \n",
    "    def detect_and_show_hint(self, code: str) -> None:\n",
    "        self.user_code = None\n",
    "        self.hint = None\n",
    "        self.detected = False\n",
    "\n",
    "        self._nodes: List[ast.AST] = []\n",
    "        self._nodes_enc: List[torch.Tensor] = []\n",
    "        self._duplicity_matrix: np.ndarray = None\n",
    "        self._duplicated_code_segments: List[DuplicatedSegment] = []\n",
    "        self._mask_same_type: np.ndarray = None\n",
    "        self._mask_same_num_lines: np.ndarray = None\n",
    "\n",
    "        try:\n",
    "            parsed = ast.parse(code)\n",
    "        \n",
    "            self._get_duplicity_matrix_base(parsed)\n",
    "            self._get_mask_same_type()\n",
    "            self._get_mask_same_num_lines()\n",
    "            self._encode_subtrees()\n",
    "            self._compute_duplicity_matrix()\n",
    "            self._scale_duplicity_matrix()\n",
    "            self._apply_mask_same_type()\n",
    "            self._apply_mask_same_num_lines()\n",
    "            self._apply_similarity_threshold()\n",
    "            self._get_duplicated_code_segments()\n",
    "            self._merge_neighbouring_code_segments()\n",
    "            self._remove_overlapping_code_segments_top_top()\n",
    "            self._remove_overlapping_code_segments_top_bottom()\n",
    "            self._remove_too_short_code_segments()\n",
    "            self._check_detected()\n",
    "                \n",
    "            if self.detected:\n",
    "                self.user_code = code\n",
    "                self._compose_hint(code)\n",
    "\n",
    "        except Exception:\n",
    "            self.user_code = None\n",
    "            self.hint = None\n",
    "            self.detected = False\n",
    "\n",
    "            # ignore solutions where the code can not be executed (due to the user indentation errors - tabs, incorrect spacing) \n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1951 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of detected errors: 365\n"
     ]
    }
   ],
   "source": [
    "runner_duplicated_code_detector = Runner(detector=DuplicatedCodeDetector())\n",
    "runner_duplicated_code_detector.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "name": "python388jvsc74a57bd0545573f70f5fa7ae16af359169e1c3e00d76a2c5b45943fccd2a2a8d4c348b6c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "545573f70f5fa7ae16af359169e1c3e00d76a2c5b45943fccd2a2a8d4c348b6c"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}